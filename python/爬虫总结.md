#### 一. 爬虫基础
##### 1. http基本原理

##### 2. request
1. request method(请求方式)
    - GET 
    - HEAD
    - POST
    - PUT	
    - DELETE
    - CONNECT
    - OPTIONS	
    - TRACE	

2. request url(请求链接)

3. request headers(请求头)
    - accept
    - accept-language
    - accept-encoding
    - host
    - cookie
    - referer
    - user-agent
    - content-type
        - application/x-www-form-urlencoded - form表达提交
        - multipart/form-data - 表单文件上传提交
        - application/json - 序列化json数据提交
        - text/xml - xml数据提交

4. request body(请求体)
一般承载的内容是 POST 请求中的 Form Data，即表单数据，而对于 GET 请求 Request Body 则为空。

##### 3. response
1. response status code
    - 301 永久重定向
    - 302 临时重定向
    - 400 错误请求
    - 401 未授权
    - 403 禁止访问
    - 501 服务器内部错误
    - 502 错误网关
    - 503 服务器不可用
    - 504 网关超时
    - 505 http版本不支持
2. response headers
    - date
    - last-modified
    - content-encoding 
    - server
    - content-type 
    - set-cookie
    - expires
3. response body

#### 二. 基本库的使用
##### 1. urllib
```python
from urllib.parse import urlencode, parse_qs, quote, unquote

data = {
    'q':'知道',
    'name':'aa'
}
url = 'https://www.baidu.com?'

# urlencode() 方法将其序列化为 URL 标准 GET 请求参数
encode_data = urlencode(data)

print(url + encode_data)
# https://www.baidu.com?q=%E7%9F%A5%E9%81%93&name=aa

print(parse_qs(encode_data))
# {'q': ['知道'], 'name': ['aa']}

# 将内容转化为 URL 编码的格式
print(quote('知道'))
# %E7%9F%A5%E9%81%93
print(unquote(r'%E7%9F%A5%E9%81%93'))
```

##### 2. requests
```python
from requests import Request, Session

url = 'http://httpbin.org/post'
data = {
    'name': 'germey'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```
##### 3. 正则表达式
#### 三. 解析库的使用
##### 1. xpath
```python
from lxml import etree
text = '''
<li class="li li-first" name="item"><a href="link.html">first item</a></li>
'''
html = etree.HTML(text)
result = html.xpath('//li[contains(@class, "li") and @name="item"]/a/text()')
print(result)
# ['first item']
```

> [使用lxml的etree.iterparse()解析大型XML](./example/使用lxml的etree.iterparse()解析大型XML.md)
> [etree.iterparse解析大型XML遇到命名空间的问题](./example/etree.iterparse解析大型XML遇到命名空间的问题.md)
> [xpath获取标签内的包括所有下级标签的所有文字内容](./example/xpath获取标签内的包括所有下级标签的所有文字内容.md)

##### 2. pyquery
```python
from pyquery import PyQuery as pq
doc = pq(html)
a = doc('a')
for item in a.items():
    print(item.attr('href'))
```
#### 四. 数据存储
##### 1. txt文本存储
##### 2. json文件存储
> [解析json文件](./example/解析json文件.md)
##### 3. csv文件存储
> [以逗号分隔字符串,但忽略双引号内的逗号](./example/以逗号分隔字符串,但忽略双引号内的逗号.md)
> [解析csv、tsv文件](./example/解析csv、tsv文件.md)
> [通过csv.writer写入数据每行都会增加一个空行](./example/通过csv.writer写入数据每行都会增加一个空行.md)
##### 4. MySQL存储
> [pymysql_example](./example/pymysql_example.md)
> [mysql_实例](../sql/mysql/mysql_实例.md)
> [mysql安装及问题](../sql/mysql/mysql安装及问题.md)

##### 5. redis存储
> [redis_实例](../sql/redis/redis_实列.md)
> [redis安装及问题](../sql/redis/redis安装及问题.md)

##### 6. mongodb存储
> [mongodb_实例](../sql/mongodb/mongo实列.md)
> [mongodb安装及问题](../sql/mongodb/mongodb安装.md)